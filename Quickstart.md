# Policy Scraper - Quick Start Guide

## âœ… Version 1.2 - Organized Output!

**Latest Updates**: 
- âœ¨ Each website gets its own folder
- ðŸ“Š Summary file with scraping statistics
- ðŸ”§ Fixed LLM URL filtering

## ðŸ“¦ What You Have

A complete, modular web scraper that:
- âœ… Uses LLM ONLY for smart URL filtering
- âœ… Uses traditional parsing for fast content extraction
- âœ… Automatically finds sitemap.xml or scrapes homepage
- âœ… Exports to JSON, TXT, and Markdown formats
- âœ… Perfect for scraping company policies, documentation, etc.

## ðŸš€ Installation (3 Steps)

### Step 1: Navigate to Project
```bash
cd policy-scraper
```

### Step 2: Run Setup Script

**Linux/Mac:**
```bash
chmod +x setup.sh
./setup.sh
```

**Windows:**
```batch
setup.bat
```

### Step 3: Configure LLM

Edit `.env` file:
```env
OLLAMA_BASE_URL=http://10.112.30.10:11434
OLLAMA_MODEL=ollama/phi4-mini-reasoning
```

## ðŸŽ¯ Run Your First Scrape

```bash
# Activate environment
source venv/bin/activate  # Linux/Mac
# OR
venv\Scripts\activate.bat  # Windows

# Scrape a website
python main.py https://www.anthropic.com

# Results will be in: scraped_data/anthropic.json
```

## ðŸ“ Project Structure

```
policy-scraper/
â”œâ”€â”€ main.py                    # â­ Main entry point
â”œâ”€â”€ .env                       # âš™ï¸ Configuration
â”œâ”€â”€ requirements.txt           # ðŸ“¦ Dependencies
â”œâ”€â”€ README.md                  # ðŸ“– Full documentation
â”œâ”€â”€ USAGE_GUIDE.md            # ðŸ“š Detailed guide
â”‚
â”œâ”€â”€ config/                    # Configuration
â”‚   â”œâ”€â”€ browser_config.py     # Browser settings
â”‚   â””â”€â”€ llm_config.py         # LLM settings
â”‚
â”œâ”€â”€ models/                    # Data schemas
â”‚   â””â”€â”€ schemas.py
â”‚
â”œâ”€â”€ scrapers/                  # Core scraping
â”‚   â”œâ”€â”€ sitemap_parser.py     # Sitemap.xml handler
â”‚   â”œâ”€â”€ url_filter.py         # LLM URL filtering
â”‚   â””â”€â”€ content_extractor.py  # Content extraction
â”‚
â”œâ”€â”€ utils/                     # Utilities
â”‚   â”œâ”€â”€ file_handler.py       # Save files
â”‚   â””â”€â”€ url_utils.py          # URL helpers
â”‚
â””â”€â”€ scraped_data/             # ðŸ’¾ Output folder
```

## ðŸŽ¨ Common Use Cases

### 1. Scrape Privacy Policies
```bash
# Edit .env
SEARCH_PROMPT=Find privacy policy, terms of service, and data protection pages.

# Run
python main.py https://www.example.com
```

### 2. Scrape Documentation
```bash
# Edit .env
SEARCH_PROMPT=Find API documentation, developer guides, and tutorials.

# Run
python main.py https://docs.example.com --format markdown
```

### 3. Batch Scrape Multiple Sites
```bash
# Create urls.txt
cat > urls.txt << EOF
https://www.anthropic.com
https://openai.com
https://www.deepmind.com
EOF

# Run
python main.py urls.txt
```

### 4. Scrape Investor Relations
```bash
# Edit .env
SEARCH_PROMPT=Find investor relations, financial reports, and annual reports.

# Run
python main.py https://www.company.com
```

## ðŸ”§ Key Features

### 1. Smart URL Discovery
- âœ… Checks for sitemap.xml first
- âœ… Falls back to homepage links
- âœ… Handles sitemap indexes

### 2. LLM Filtering
- âœ… AI selects relevant URLs only
- âœ… Customizable search prompts
- âœ… Fallback to keyword matching

### 3. Clean Content Extraction
- âœ… Removes navigation, footers, ads
- âœ… Auto-detects page types
- âœ… Counts words and metadata

### 4. Multiple Output Formats
- âœ… JSON for data processing
- âœ… TXT for reading
- âœ… Markdown for documentation

## ðŸ“Š Example Output

After running:
```bash
python main.py https://www.anthropic.com
```

You get:
```
scraped_data/
â””â”€â”€ anthropic/              # Website-specific folder
    â”œâ”€â”€ anthropic.json      # Structured data
    â”œâ”€â”€ anthropic.txt       # Readable text
    â”œâ”€â”€ anthropic.md        # Markdown format
    â””â”€â”€ summary.txt         # Scraping statistics â­ NEW
```

**anthropic.json:**
```json
[
  {
    "url": "https://www.anthropic.com/privacy",
    "title": "Privacy Policy",
    "page_type": "Privacy Policy",
    "content": "Full text...",
    "word_count": 2341
  }
]
```

**summary.txt:**
```
ðŸ“Š SCRAPING SUMMARY
================================================================================
Website: https://www.anthropic.com
Scraped: 2024-01-29 10:30:00
URLs Discovered: 150
Relevant URLs: 8
Pages Scraped: 8
Total Words: 15,234

Page Types:
  - Privacy Policy: 1
  - Terms of Service: 1
================================================================================
â±ï¸  Total time: 45.23 seconds
```

## âš™ï¸ Customization

### Change What URLs to Find

Edit `.env`:
```env
# For policies (default)
SEARCH_PROMPT=Find company policies, privacy, terms, legal pages.

# For blog posts
SEARCH_PROMPT=Find blog posts, articles, and news.

# For careers
SEARCH_PROMPT=Find career pages, job listings, and benefits.
```

### Change Browser Visibility

Edit `config/browser_config.py`:
```python
headless=False  # See browser during scraping
```

### Change Output Format

```bash
python main.py URL --format json      # JSON only
python main.py URL --format text      # TXT only
python main.py URL --format markdown  # MD only
python main.py URL --format all       # All formats (default)
```

## ðŸ› Troubleshooting

### "No relevant URLs found"
â†’ Adjust SEARCH_PROMPT in .env to be more general

### "LLM connection failed"
â†’ Check OLLAMA_BASE_URL and verify Ollama is running

### "Empty content extracted"
â†’ Set headless=False in browser_config.py to debug

### "Module not found"
â†’ Make sure you activated venv: `source venv/bin/activate`

## ðŸ“š Documentation Files

1. **README.md** - Complete overview and features
2. **USAGE_GUIDE.md** - Detailed usage instructions
3. **This file** - Quick start guide

## ðŸŽ¯ Next Steps

1. âœ… Run setup script
2. âœ… Configure .env
3. âœ… Try first scrape: `python main.py https://www.anthropic.com`
4. âœ… Check output in `scraped_data/`
5. âœ… Customize SEARCH_PROMPT for your needs
6. âœ… Process batch URLs with urls.txt

## ðŸ’¡ Pro Tips

1. **Test with one URL first** before batch processing
2. **Review outputs** to verify quality
3. **Use sitemap when available** - much faster
4. **Customize search prompts** for each use case
5. **Save as JSON** if you only need structured data

---

## Command Cheat Sheet

```bash
# Setup
./setup.sh                           # Linux/Mac setup
setup.bat                            # Windows setup

# Activate environment
source venv/bin/activate             # Linux/Mac
venv\Scripts\activate.bat            # Windows

# Run scraper
python main.py URL                   # Single URL
python main.py urls.txt              # Multiple URLs
python main.py URL --format json     # JSON only

# Customize
nano .env                            # Edit configuration
nano config/browser_config.py        # Browser settings
```

---

Happy scraping! ðŸ•·ï¸

For detailed information, see:
- **README.md** - Full documentation
- **USAGE_GUIDE.md** - Advanced usage and examples